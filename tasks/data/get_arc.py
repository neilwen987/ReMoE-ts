#!/usr/bin/env python
"""Download the AI2 ARC dataset using the ğŸ¤— datasets library
and save it to a local directory for downstream evaluation.

ç”¨æ³•ï¼š
    python tasks/data/get_arc.py \
        --subset arc-e \
        --split validation \
        --output_dir tasks/data/arc

å‚æ•°è¯´æ˜ï¼š
    --subset:      ARCå­é›†ï¼Œå¯é€‰ [arc-e, arc-c]
    --split:       è¦ä¸‹è½½çš„æ•°æ®åˆ’åˆ†ï¼Œå¯é€‰ [train, validation, test]
    --output_dir:  ä¿å­˜æ•°æ®é›†çš„æœ¬åœ°ç›®å½•ã€‚

ä¾èµ–ï¼špip install datasets tqdm
"""
import argparse
import json
import os
from pathlib import Path

from datasets import load_dataset
from datasets import disable_caching


def parse_args():
    parser = argparse.ArgumentParser(description="Download AI2 ARC dataset")
    parser.add_argument(
        "--subset",
        default='ARC-Challenge',
        choices=['ARC-Challenge', 'ARC-Easy'],
        help="ARCå­é›† (arc-e/arc-c)ã€‚",
    )
    parser.add_argument(
        "--split",
        default="validation",
        choices=["train", "validation", "test"],
        help="è¦ä¸‹è½½çš„æ•°æ®é›†åˆ’åˆ† (train/validation/test)ã€‚",
    )
    parser.add_argument(
        "--output_dir",
        default="./arc",
        help="ä¿å­˜æ•°æ®é›†çš„æœ¬åœ°ç›®å½•ã€‚",
    )
    return parser.parse_args()


def save_arc_dataset_as_json(dataset, output_dir, subset_name):
    """
    å°†ARCæ•°æ®é›†ä¿å­˜ä¸ºé¡¹ç›®ä¸­ä½¿ç”¨çš„JSONæ ¼å¼ã€‚
    æ¯ä¸ªæ•°æ®é›†åˆ†å‰²å°†ä¿å­˜ä¸ºä¸€ä¸ªç‹¬ç«‹çš„JSONæ–‡ä»¶ã€‚
    
    Args:
        dataset: æ•°æ®é›†å¯¹è±¡
        output_dir: ä¿å­˜JSONæ–‡ä»¶çš„æ ¹ç›®å½•
        subset_name: å­é›†åç§° (arc-e æˆ– arc-c)
    """
    os.makedirs(output_dir, exist_ok=True)
    
    # ä¸ºæ¯ä¸ªsplitåˆ›å»ºä¸€ä¸ªå­ç›®å½•
    split_dir = os.path.join(output_dir, subset_name)
    os.makedirs(split_dir, exist_ok=True)
    
    all_samples_for_split = []
    
    for sample in dataset:
        # è½¬æ¢ä¸ºé¡¹ç›®ä¸­ä½¿ç”¨çš„æ ¼å¼
        data = {
            "question": sample["question"],
            "choices": sample["choices"]["text"],  # ARCçš„é€‰é¡¹æ ¼å¼
            "answerKey": sample["answerKey"],      # æ­£ç¡®ç­”æ¡ˆçš„key
            "id": sample.get("id", ""),
            "category": sample.get("category", "")
        }
        all_samples_for_split.append(data)
    
    # å®šä¹‰è¾“å‡ºæ–‡ä»¶åå’Œè·¯å¾„
    filename = f"{subset_name}.json"
    filepath = os.path.join(split_dir, filename)
    
    # ä¿å­˜ä¸ºJSONæ–‡ä»¶
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(all_samples_for_split, f, ensure_ascii=False, indent=4)
    
    print(f"å·²å°† '{subset_name}' åˆ†å‰²ä¿å­˜åˆ° '{filepath}'")
    print(f"æ€»å…± {len(all_samples_for_split)} ä¸ªæ ·æœ¬")


def main():
    args = parse_args()
    # å…³é—­ datasets çš„ç¼“å­˜ä»¥é¿å…æ··æ·†
    disable_caching()

    print(f"å¼€å§‹ä¸‹è½½ AI2 ARC ({args.subset}, {args.split}) â€¦")
    
    # åŠ è½½ARCæ•°æ®é›†
    dataset = load_dataset("ai2_arc", args.subset, split=args.split)

    output_dir = Path(args.output_dir).expanduser().resolve()
    output_dir.mkdir(parents=True, exist_ok=True)

    print(f"ä¿å­˜åˆ° {output_dir} â€¦")
    save_arc_dataset_as_json(dataset, output_dir, args.subset)
    print("å®Œæˆï¼")


if __name__ == "__main__":
    main()
